{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN Training and Inference\n",
    "In this notebook we use Matterport's implementation of Mask R-CNN to train on a custom dataset, then use the trained weights to run inference on new images.\n",
    "\n",
    "This is part of a tutorial posted at http://www.immersivelimit.com/tutorials/using-mask-r-cnn-on-custom-coco-like-dataset, so check there for more details.\n",
    "\n",
    "### Not a beginner tutorial...\n",
    "This is not intended to be a complete beginner tutorial.\n",
    "- You should know how to clone a git repository from GitHub\n",
    "- You should have a decent understanding of Python programming\n",
    "- You should understand the basics of training deep neural networks. If you've never done it before, I wouldn't recommend starting here. Find a beginner deep learning tutorial/course and start there. There's lots of great free stuff on YouTube.\n",
    "\n",
    "## **Download the Free Training Dataset First!\n",
    "I've provided a free training dataset of cigarette butt images for use with this tutorial.\n",
    "1. Download it from here: http://www.immersivelimit.com/datasets/cigarette-butts\n",
    "2. Then extract the files and put it in the \"../datasets\" directory of this repo\n",
    "3. The code below will be looking for \"../datasets/cig_butts/train\", \"../datasets/cig_butts/val\", and \"../datasets/cig_butts/real_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Matterport's \"mrcnn\" libraries\n",
    "1. I'm using a forked version of Matterport's Mask_RCNN git repo to make sure this code works, even if the main repo changes. As of July 29, 2018, there is a bug in Matterport's version that breaks loading an existing model that I've fixed in my version (and submitted a pull request to fix it in theirs).\n",
    "  - My version is here: https://github.com/akTwelve/Mask_RCNN\n",
    "  - The original is here: https://github.com/matterport/Mask_RCNN\n",
    "2. Update the code cell below to point to the root directory of the repo on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = '../../../aktwelve_Mask_RCNN'\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    "\n",
    "# Import mrcnn libraries\n",
    "# sys.path.append(ROOT_DIR) \n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "import mrcnn.model as modellib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logging and pre-trained model paths\n",
    "This will default to sub-directories in your mask_rcnn_dir, but if you want them somewhere else, updated it here.\n",
    "\n",
    "It will also download the pre-trained coco model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Define configurations for training on the Cigarette Butt dataset.\n",
    "### Note\n",
    "These are settings that worked on my machine (GTX 970 graphics card). If you are getting OOM (Out of Memory) errors, you may need to tweak the settings or your computer may not be powerful enough. If you have a better graphics card, you will want to tweak it to take advantage of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CigButtsConfig(Config):\n",
    "    \"\"\"Configuration for training on the cigarette butts dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the cigarette butts dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"cig_butts\"\n",
    "\n",
    "    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 1 (cig_butt)\n",
    "\n",
    "    # All of our training images are 512x512\n",
    "    IMAGE_MIN_DIM = 257\n",
    "    IMAGE_MAX_DIM = 257\n",
    "\n",
    "    # You can experiment with this number to see if it improves training\n",
    "    STEPS_PER_EPOCH = 500\n",
    "\n",
    "    # This is how often validation is run. If you are using too much hard drive space\n",
    "    # on saved models (in the MODEL_DIR), try making this value larger.\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "    # Matterport'../../../Mask_RCNN' originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    BACKBONE = 'resnet50'\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    MAX_GT_INSTANCES = 50 \n",
    "    POST_NMS_ROIS_INFERENCE = 500 \n",
    "    POST_NMS_ROIS_TRAINING = 1000 \n",
    "    \n",
    "config = CigButtsConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dataset\n",
    "I've attempted to make this generic to any COCO-like dataset. That means if you have another dataset defined in the COCO format, it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoLikeDataset(utils.Dataset):\n",
    "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
    "        See http://cocodataset.org/#home for more information.\n",
    "    \"\"\"\n",
    "    def load_data(self, annotation_json, images_dir):\n",
    "        \"\"\" Load the coco-like dataset from json\n",
    "        Args:\n",
    "            annotation_json: The path to the coco annotations json file\n",
    "            images_dir: The directory holding the images referred to by the json file\n",
    "        \"\"\"\n",
    "        # Load json from file\n",
    "        json_file = open(annotation_json)\n",
    "        coco_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        \n",
    "        # Add the class names using the base method from utils.Dataset\n",
    "        source_name = \"coco_like\"\n",
    "        for category in coco_json['categories']:\n",
    "            class_id = category['id']\n",
    "            class_name = category['name']\n",
    "            if class_id < 1:\n",
    "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
    "                return\n",
    "            \n",
    "            self.add_class(source_name, class_id, class_name)\n",
    "        \n",
    "        # Get all annotations\n",
    "        annotations = {}\n",
    "        for annotation in coco_json['annotations']:\n",
    "            image_id = annotation['image_id']\n",
    "            if image_id not in annotations:\n",
    "                annotations[image_id] = []\n",
    "            annotations[image_id].append(annotation)\n",
    "        \n",
    "        # Get all images and add them to the dataset\n",
    "        seen_images = {}\n",
    "        for image in coco_json['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in seen_images:\n",
    "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                seen_images[image_id] = image\n",
    "                try:\n",
    "                    image_file_name = image['file_name']\n",
    "                    image_width = image['width']\n",
    "                    image_height = image['height']\n",
    "                except KeyError as key:\n",
    "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
    "                \n",
    "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
    "                image_annotations = annotations[image_id]\n",
    "                \n",
    "                # Add the image using the base method from utils.Dataset\n",
    "                self.add_image(\n",
    "                    source=source_name,\n",
    "                    image_id=image_id,\n",
    "                    path=image_path,\n",
    "                    width=image_width,\n",
    "                    height=image_height,\n",
    "                    annotations=image_annotations\n",
    "                )\n",
    "                \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\" Load instance masks for the given image.\n",
    "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
    "        Args:\n",
    "            image_id: The id of the image to load masks for\n",
    "        Returns:\n",
    "            masks: A bool array of shape [height, width, instance count] with\n",
    "                one mask per instance.\n",
    "            class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "        annotations = image_info['annotations']\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            class_id = annotation['category_id']\n",
    "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
    "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
    "            for segmentation in annotation['segmentation']:\n",
    "                mask_draw.polygon(segmentation, fill=1)\n",
    "                bool_array = np.array(mask) > 0\n",
    "                instance_masks.append(bool_array)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        mask = np.dstack(instance_masks)\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        \n",
    "        return mask, class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training and Validation Datasets\n",
    "Make sure you download the training dataset linked at the top of this notebook. If you put the dataset somewhere else, update the paths below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CocoLikeDataset()\n",
    "dataset_train.load_data('../ANNOTATIONS/grid_2.json', '../IMGS/CORTES/')\n",
    "dataset_train.prepare()\n",
    "\n",
    "# dataset_val = CocoLikeDataset()\n",
    "# dataset_val.load_data('../datasets/cig_butts/val/coco_annotations.json', '../datasets/cig_butts/val/images')\n",
    "# dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a few images from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_train\n",
    "image_ids = np.random.choice(dataset.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    dataset.load_image(image_id)\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training Model and Train\n",
    "This code is largely borrowed from the train_shapes.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass layers='heads' to the train() function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass layers=\"all to train all layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "start_train = time.time()\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=4, \n",
    "            layers='heads')\n",
    "end_train = time.time()\n",
    "minutes = round((end_train - start_train) / 60, 2)\n",
    "print(f'Training took {minutes} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "start_train = time.time()\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=8, \n",
    "            layers=\"all\")\n",
    "end_train = time.time()\n",
    "minutes = round((end_train - start_train) / 60, 2)\n",
    "print(f'Training took {minutes} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to run Inference\n",
    "Create a new InferenceConfig, then use it to create a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(CigButtsConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    DETECTION_MIN_CONFIDENCE = 0.85\n",
    "    \n",
    "\n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "Run model.detect() on real images.\n",
    "\n",
    "We get some false positives, and some misses. More training images are likely needed to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "real_test_dir = '../../../datasets/cig_butts/real_test/'\n",
    "image_paths = []\n",
    "for filename in os.listdir(real_test_dir):\n",
    "    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
    "        image_paths.append(os.path.join(real_test_dir, filename))\n",
    "\n",
    "for image_path in image_paths:\n",
    "    img = skimage.io.imread(image_path)\n",
    "    img_arr = np.array(img)\n",
    "    results = model.detect([img_arr], verbose=1)\n",
    "    r = results[0]\n",
    "    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset_val.class_names, r['scores'], figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
